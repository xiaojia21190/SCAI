<?xml version="1.0" encoding="utf-8" ?>
<svg baseProfile="full" height="400px" version="1.1" width="800px" xmlns="http://www.w3.org/2000/svg" xmlns:ev="http://www.w3.org/2001/xml-events" xmlns:xlink="http://www.w3.org/1999/xlink"><defs /><rect fill="black" height="100%" width="100%" x="0" y="0" /><image height="150" width="150" x="20" xlink:href="bird.png" y="100" /><text fill="white" style="font-family:Alexandria Regular; font-size:30px; font-weight:bold;" text-anchor="middle" x="400" y="50">REFERENCE</text><text fill="white" style="font-family:Alexandria Thin; font-size:14px;" text-anchor="start" x="200" y="100">1. Vaswani, A. (2017). Attention is all you need. Advances in Neural Information</text><text fill="white" style="font-family:Alexandria Thin; font-size:14px;" text-anchor="start" x="200" y="120">Processing Systems.</text><text fill="white" style="font-family:Alexandria Thin; font-size:14px;" text-anchor="start" x="200" y="150">2. Bahdanau, D. (2014). Neural machine translation by jointly learning to align and</text><text fill="white" style="font-family:Alexandria Thin; font-size:14px;" text-anchor="start" x="200" y="170">translate. arXiv preprint arXiv:1409.0473.</text><text fill="white" style="font-family:Alexandria Thin; font-size:14px;" text-anchor="start" x="200" y="200">3. Luong, M. T. (2015). Effective approaches to attention-based neural machine</text><text fill="white" style="font-family:Alexandria Thin; font-size:14px;" text-anchor="start" x="200" y="220">translation. arXiv preprint arXiv:1508.04025.</text><text fill="white" style="font-family:Courier New; font-size:14px; font-weight:bold;" text-anchor="middle" x="95" y="270">@SCAI_Agent</text><text fill="white" style="font-family:Courier New; font-size:14px;" x="200" y="320">Multiple agents in SCAI collaborate to ensure reliable, evidence-based</text><text fill="white" style="font-family:Courier New; font-size:14px;" x="200" y="340">answers through literature search and analysis.</text></svg>